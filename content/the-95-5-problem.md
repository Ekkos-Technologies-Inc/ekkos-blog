---
title: "The 95-5 Problem: AI Writes 90% of Your Code But Remembers 0% of Why"
description: "AI coding agents are shifting developers from writing to reviewing. But without persistent memory, every session starts from scratch -- and the hard part gets harder."
date: "2026-02-10T10:00:00-05:00"
author: "ekkOS Team"
tags: ["ai-agents", "developer-productivity", "context-loss", "code-review"]
image: "/images/blog/the-95-5-problem.png"
imageAlt: "A developer surrounded by AI-generated code streams, reading intently while context fades behind them"
draft: true
---

A year ago, David Crawshaw -- the former CTO of Tailscale -- estimated that AI wrote about 25% of his code. As of February 2026, he puts the number at roughly 90%. His reading-to-writing ratio has shifted from 50-50 to what he calls "95-5": he spends nearly all of his time reviewing code rather than writing it.

This is not an isolated anecdote. Across developer communities, the pattern is consistent: frontier models have gotten good enough that the bottleneck is no longer generation. It is evaluation. And evaluation, it turns out, depends on something most AI tools do not have -- memory of why things are the way they are.

## The investigation paradox

A recent essay that reached the top of Hacker News (522 points as of this writing) frames the problem sharply: AI makes the easy part easier and the hard part harder. The author, a working developer, describes a specific failure mode [OBSERVED: "AI Makes the Easy Part Easier and the Hard Part Harder," blundergoat.com, February 2026]:

> If we skipped the investigation because AI already gave us an answer, you don't have the context to evaluate what it gave you.

This is the investigation paradox. When a developer writes code manually, the act of writing builds context. You learn the shape of the problem. You discover edge cases. You develop opinions about trade-offs. When an AI agent writes that same code, the developer inherits the output but not the understanding.

The result is that code review -- the "hard part" that now consumes 95% of developer time in AI-heavy workflows -- becomes harder, not easier. You are reading code you did not write, generated by a system that cannot explain its reasoning beyond what you prompt it to explain, in a context that resets every session.

## Where context disappears

Most AI coding agents operate in what amounts to a persistent amnesia loop. Each session, the agent:

1. Receives a context window (somewhere between 128K and 1M tokens, depending on the model)
2. Works within that window for the duration of the session
3. Discards everything when the session ends

The practical consequence is that decisions made in session 1 have no influence on session 5. The agent does not know that you chose a particular database schema for performance reasons. It does not remember that a previous approach caused a production incident. It cannot recall that the team decided against a specific library due to licensing constraints.

This creates a compounding problem. In Crawshaw's workflow, using frontier models like Opus is essential because cheaper alternatives "teach the wrong lessons about capabilities" [OBSERVED: Crawshaw, "Eight More Months of Agents," February 2026]. But even frontier models operating without persistent context are starting each session blind. The more code the agent writes, the more context the developer must reconstruct manually.

## The 400-line deletion problem

The blundergoat essay describes a concrete failure: an AI agent deleted 400 lines from a file while claiming to add a test. The developer had to use git to recover the code -- a process that likely took longer than writing the test manually would have.

This is not a model capability failure. Modern frontier models are quite good at code generation. It is a context failure. The agent did not know:

- What those 400 lines were for
- Why they were structured that way
- What would break if they disappeared
- That a previous session had specifically organized that file to avoid this kind of collision

A developer with institutional memory of the codebase would never make this mistake. Not because they are smarter than the model, but because they carry context that the model does not.

## The burnout acceleration loop

The Berkeley Haas study of 200 tech employees published in Harvard Business Review this month identifies a related mechanism [OBSERVED: "AI Doesn't Reduce Work -- It Intensifies It," HBR, February 2026]. When AI tools enable teams to deliver quickly, management calibrates expectations upward. The result is a cycle:

1. AI helps the team ship faster
2. Management expects faster delivery as the new baseline
3. Tired engineers skip tests and context-building
4. Defects increase, creating rework
5. AI is used to fix the defects it helped introduce

The study found three specific mechanisms of work intensification: task expansion (developers taking on work they previously outsourced), blurred boundaries (work seeping into breaks and evenings), and increased multitasking (managing multiple active threads simultaneously).

Each of these mechanisms is amplified by context loss. When the AI agent forgets what happened yesterday, the developer must re-explain. When the developer is juggling multiple threads, re-explaining is the first thing that gets skipped. The result is lower-quality prompts, lower-quality output, and more time spent on review and repair.

## What the 95-5 workflow actually needs

If 95% of developer time is now spent reading and evaluating AI-generated code, the tooling should optimize for that activity. In practice, most AI coding tools optimize for generation speed and model capability -- important, but insufficient.

The missing layer is what you might call "decision persistence." For the 95-5 workflow to function well, the system needs to carry forward:

- **Architectural decisions and their rationale** -- Why this pattern, not that one
- **Past failure modes** -- What went wrong before and how it was fixed
- **Team conventions** -- Naming, structure, testing expectations
- **Dependency context** -- What was chosen, what was rejected, and why
- **Session continuity** -- What the agent was doing yesterday, and where it left off

Without this, every review session starts with the developer rebuilding context from scratch. With 10-20 agent interactions per prompt (a common pattern in tool-using agents), this context reconstruction happens continuously.

## Three approaches to the problem (with trade-offs)

**1. Extended context windows**

The obvious approach: make the window bigger so more history fits. Models now support 128K to 1M tokens, and some providers are experimenting with even larger windows.

*Trade-off:* Larger windows increase cost linearly and latency sub-linearly. At 1M tokens, a single API call can cost several dollars. More importantly, "in the window" does not mean "attended to" -- models exhibit well-documented degradation on information in the middle of long contexts [OBSERVED: "Lost in the Middle," Liu et al., 2023]. Stuffing more history into the window helps, but with diminishing returns.

**2. RAG over conversation history**

Retrieve relevant past conversations using embedding similarity when the agent starts a new session.

*Trade-off:* Retrieval quality depends heavily on the embedding model and chunking strategy. Conversation turns are often poor retrieval units because they mix intent, code, and commentary. A developer saying "actually, let's go back to the other approach" is contextually meaningful but semantically vague. Many RAG implementations surface text that is lexically similar but contextually wrong.

**3. Persistent structured memory**

Maintain a dedicated memory layer that captures decisions, outcomes, and patterns -- not raw conversation text, but distilled knowledge about what worked, what failed, and why.

*Trade-off:* This requires someone (or something) to do the distillation. Automated extraction is noisy. Manual curation does not scale. The feedback loop between "pattern stored" and "pattern actually helped" is difficult to close, and without it, the memory accumulates noise over time.

## How we think about this at ekkOS_

We approach this by tracking outcome success rates for every pattern stored. When a pattern helps solve a problem, its weight increases; when it fails or gets overridden, it decreases. Over time, the system develops a weighted knowledge base that reflects what actually works in a specific codebase -- not just what was discussed. This requires users to close the feedback loop, which adds friction but prevents the memory from becoming a graveyard of stale context. If you are evaluating memory systems for your AI workflow, the question to ask is: "Does this system know which of its own suggestions actually worked?"

## What to measure

If you are experiencing the 95-5 problem, here is a practical checklist for evaluating whether memory tooling would help:

- [ ] **Track re-explanation frequency.** How often do you re-describe the same architectural decision to your AI agent? If the answer is "most sessions," that is context loss you could recover.
- [ ] **Count context-free errors.** When your agent makes a mistake, ask: "Would a developer who was here yesterday have made this mistake?" If the answer is no, the issue is memory, not capability.
- [ ] **Measure session startup cost.** Time how long it takes from "start new session" to "agent is productive on my codebase." If this is more than a few minutes, there is a context gap.
- [ ] **Audit your prompts.** Look at the last 20 prompts you gave your AI agent. How many include context that the agent should already know? That is your memory tax.

The 95-5 shift is real, and in many ways it is progress. Developers should be spending their time on understanding and evaluation, not boilerplate generation. But the tooling has not caught up to the workflow. The hard part got harder, and the tools that could help are still, in many setups, resetting to zero every session.
